{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Volatility of Asset Prices With Google Search Volume\n",
    "\n",
    "In this notebook, we will use Google search volume data to build proxies for predicting the volatility of market securities. All you need to run this code, is an initial set of keywords related to the security of interest. Make sure you have the following packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!require(xts, tseries, lmtest, fGarch)){ \n",
    "  install.packages('xts', 'tseries', 'lmtest', 'fGarch')}\n",
    "if (!require(dplyr, sandwich, gtrendsR, BatchGetSymbols)) {\n",
    "  install.packages('dplyr', 'sandwich', 'gtrendsR', 'BatchGetSymbols')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Trend Data\n",
    "\n",
    "To obtain reliable results with sufficient data, we would need to use data with weekly frequency. However, as of right now, [Google Trends](https://trend.google.com) allows downloading weekly data for only five years, and therefore our initial dataset would be limited. As these data are normalized and are not absolute search volumes, combining different data sets is not possibles. To overcome this issue, there are suggestions to use a combination of daily and monthly (or weekly) data and standardizing the daily data according to the monthly record, but to the best of my information, most of them yield to unstable results. Therefore, the present analysis would be limited to only five years worth of data.\n",
    "\n",
    "Trends data can be downloaded directly using the ```gtrendsR``` package in R. Note that, as the data in Google Trends is normalized, we should download the data for each keyword *separately*, especially if our initial dataset contains more than five keywords, which is the maximum number of terms for which Google Trend returns data in a single query.\n",
    "\n",
    "The example provided here uses *GDP*, *Unemployment*, *Interest Rate*, *5G*, *iPhone*, *Facebook*, *Google*, *Apple*, *FAANG*, *Artificial Intelligence*, and *Gold Price* as the keywords of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(gtrendsR)\n",
    "\n",
    "terms <- c (\"GDP\", \"Unemployment\", \"Interest Rate\",\n",
    "            \"5G\", \"iPhone\", \"Facebook\", \"Google\",\n",
    "            \"Apple\", \"FAANG\", \"Artificial Intelligence\", \"Gold Price\") #example\n",
    "\n",
    "Trend <- function(terms, loc){\n",
    "  TrendList <- list()\n",
    "  i=1\n",
    "  for (term in terms){\n",
    "    df <- gtrends(keyword = term, geo = loc, time = \"today+5-y\", onlyInterest = T)\n",
    "    gsv <- data.frame (df$interest_over_time)\n",
    "    if (i == 1) {TrendList[[i]] <- as.Date(gsv$date, origin = \"1970-01-01\")}\n",
    "    TrendList[[i+1]] <- gsv$hits\n",
    "    i = i + 1\n",
    "  }\n",
    "  trends <- t(data.frame(matrix(unlist(TrendList), nrow=length(TrendList), byrow=T)))\n",
    "  colnames(trends) <- c(\"Date\", terms)\n",
    "  trends <- data.frame(trends)\n",
    "  trends$Date <- as.Date(trends$Date, origin = \"1970-01-01\")\n",
    "  return(trends)\n",
    "}\n",
    "\n",
    "trends <- Trend(terms, 'US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Data\n",
    "\n",
    "Having stored the GSV as a dataframe, we can move on to downloading market data. There are several packages such as ```quantmod``` or ```BatchGetSymbols``` which allow for obtaining these data directly in R. In this notebook, I will use the ```BatchGetSymbols``` package. (See the package documentation for more information.) \n",
    "\n",
    "The two assets used as an example are *Apple* and *Facebook* with *AAPL* and *FB* ticker symbols. The function ```price``` returns daily price and returns for the ticker symobls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers <- c (\"AAPL\", \"FB\") #Example\n",
    "\n",
    "Price <- function(tickers){\n",
    "  first.date <- min(trends$Date)-7 # or manually enter dates\n",
    "  last.date <- max(trends$Date)\n",
    "  freq.data <- 'daily'\n",
    "  l.out <- BatchGetSymbols(tickers      = tickers, \n",
    "                           first.date   = first.date,\n",
    "                           last.date    = last.date, \n",
    "                           freq.data    = freq.data,\n",
    "                           cache.folder = file.path(tempdir(), \n",
    "                                                    'input-data') )\n",
    "  df <- l.out$df.tickers\n",
    "  df.wide <- reshape.wide(df)\n",
    "  prices <- df.wide$price.close\n",
    "  returns <- df.wide$ret.closing.prices\n",
    "  colnames(prices)[1] <- c(\"Date\")\n",
    "  colnames(returns)[1] <- c(\"Date\")\n",
    "  returns <- na.omit(returns)\n",
    "  result = list()\n",
    "  result$price <- prices\n",
    "  result$returns <- returns\n",
    "  return(result)\n",
    "}\n",
    "StockData <- Price (tickers)\n",
    "returns <- StockData$returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After storing the return data, we can calculate the weekly volatility, which is our main variable of interest. (Note that when combining the two data sets, only one date column is kept.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xts)\n",
    "colSd <- function (x, na.rm=FALSE) {apply(X=x, MARGIN=2,\n",
    "                                          FUN=sd, na.rm=na.rm)}\n",
    "returns <- as.xts(returns[, 2:ncol(returns)], order.by = as.Date(returns$Date))\n",
    "volatilities <- apply.weekly(returns, colSd)\n",
    "volatilities <- data.frame(Date=index(volatilities),\n",
    "                           coredata(volatilities))\n",
    "df <- cbind(volatilities, trends[,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filteration process and Building Proxies\n",
    "\n",
    "The methodolgy used in this paper is that of [Google search keywords that best predict energy price volatility](https://www.sciencedirect.com/science/article/pii/S0140988317302517) and I will avoid going through the technical details here (A summary of that paper can be accessed in [this notebook](https://github.com/moafkhami/Google-Search-Volatility/blob/master/Google-Search-Energy.ipynb)). Rather, I will mention the filtering process and its execution for buliding proxies for each market security. The process in brief is as follows:\n",
    "- Make sure the search volume time series Granger Cause volatlity.\n",
    "- Include as an explanatory variable in conventional GARCH models and see if they have predictive power beyond GARCH. Criteria for judgement are\n",
    " * F-Test\n",
    " * Significant coefficient estimate\n",
    " * Improved adjusted-$R^2$\n",
    "- Test if the time series of combinations of these keywords enhances prediction.\n",
    "\n",
    "At each step, keywords that do not meet the criteria are omitted from the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lmtest)\n",
    "GrangerTable <- matrix(NA, nrow = ncol(trends)-1, ncol = ncol(volatilities)-1)\n",
    "for (i in 1:nrow(GrangerTable)){\n",
    "  for (j in 1:ncol(GrangerTable)){\n",
    "    pval = grangertest(df[,j+1] ~ df[,i+ncol(GrangerTable)+1], order=2, data=df)$Pr[2]\n",
    "    GrangerTable[i,j] = ifelse(pval < 0.05, signif(pval, digits = 2), \"--\")\n",
    "  }\n",
    "}\n",
    "colnames(GrangerTable) <- colnames(volatilities)[-1]\n",
    "rownames(GrangerTable) <- colnames(trends)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garch estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WeeklyRet <- apply.weekly(returns, mean)\n",
    "\n",
    "GarchEst <- matrix(NA, nrow = 2*ncol(WeeklyRet), ncol = 7) #GARCH (1,1) model\n",
    "\n",
    "#Residual and cond. var. extraction\n",
    "Params <- matrix(NA, nrow = nrow(WeeklyRet), ncol = 2*ncol(WeeklyRet))\n",
    "for (i in 1:ncol(WeeklyRet)){\n",
    "  fitted <- garchFit(formula = ~garch(1,1), data = WeeklyRet[,i], cond.dist=\"std\")\n",
    "  GarchEst[2*i-1, 1] = colnames(WeeklyRet)[i]\n",
    "  GarchEst[2*i-1, 2:5] = fitted@fit$coef[1:4]\n",
    "  GarchEst[2*i, 2:5] = paste(\"(\",fitted@fit$tval[1:4], \")\")\n",
    "  GarchEst[2*i-1, 6] = fitted@fit$llh\n",
    "  GarchEst[2*i-1, 7] = fitted@fit$ics[1]\n",
    "  Params[, 2*i-1] = log(residuals(fitted)^2)\n",
    "  Params[, 2*i] = Params[, 2*i] = fitted@h.t*1000\n",
    "  #Extracting residuals and conditional variance\n",
    "}\n",
    "GarchEst[is.na(GarchEst)] <- \"\"\n",
    "colnames(GarchEst) <- c(\"Commodity\", \"mu\", \"omega\", \"alpha\", \"beta\", \"logL\", \"AIC\" )\n",
    "#xtable(GarchEst)\n",
    "colnames(Params) <- c(1:ncol(Params))\n",
    "\n",
    "for (i in 1:ncol(WeeklyRet)){\n",
    "  colnames(Params)[2*i-1] <- paste0(\"s_\", colnames(WeeklyRet)[i], sep='')\n",
    "  colnames(Params)[2*i] <- paste0(\"v_\", colnames(WeeklyRet)[i], sep='')\n",
    "}\n",
    "\n",
    "df <- cbind (Params[-1,], trends[-ncol(trends),]) #using lagged trends\n",
    "\n",
    "library(dplyr)\n",
    "df <- df %>%\n",
    "  select(Date, everything())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords as Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lmtest)\n",
    "library(sandwich)\n",
    "onelist <- list()\n",
    "twolist <- list()\n",
    "threelist <- list()\n",
    "k1 = k2 = k3 = 1\n",
    "for (i in 1:ncol(WeeklyRet)){\n",
    "  z1 <- lm(df[,2*i] ~ df[,2*i+1], data = df) #original regression\n",
    "  sumz1 <- summary(z1)\n",
    "  err1 <- coeftest(z1, vcov = vcovHC(z1, type=\"HC1\")) #White Errors\n",
    "  for (j in 2:ncol(trends)){\n",
    "    if (GrangerTable[j - 1, i]!= \"--\"){ #Granger causality condition\n",
    "      z2 <- lm(df[,2*i] ~ df[,2*i + 1] + df[,2*ncol(WeeklyRet) + j], data = df)\n",
    "      sumz2 <- summary(z2) #new equation\n",
    "      err2 <- coeftest(z2, vcov = vcovHC(z2, type=\"HC1\"))\n",
    "      if (anova(z1, z2)$Pr[2] <= 0.05 && err2[3,4] <= 0.05\n",
    "          && sumz2$adj.r.squared > 1.1* sumz1$adj.r.squared){ #enhancement requirements\n",
    "        dat1 = c(colnames(WeeklyRet)[i], colnames(trends[j]),\n",
    "                 signif(sumz2$coefficients[1:3,1], 3),\n",
    "                 signif(sumz2$coefficients[1:3,3], 3),\n",
    "                 signif(sumz2$adj.r.squared, 3)) \n",
    "        onelist[[k1]] = dat1 #append\n",
    "        k1 = k1 + 1\n",
    "        for (l in (j+1):ncol(trends)){ #two keywords\n",
    "          z3 <- lm(df[,2*i] ~ df[,2*i+1] +\n",
    "                     df[,2*ncol(WeeklyRet) + j] + df[,2*ncol(WeeklyRet) + l] ,\n",
    "                   data = df)\n",
    "          sumz3 <- summary(z3)\n",
    "          err3 <- coeftest(z3, vcov = vcovHC(z3, type=\"HC1\"))\n",
    "          if (anova(z2, z3)$Pr[2] <= 0.05 && err3[4,4] <= 0.05\n",
    "              && sumz3$adj.r.squared > 1.1* sumz2$adj.r.squared){ \n",
    "            imp=(100*(sumz3$adj.r.squared-sumz1$adj.r.squared))/sumz1$adj.r.squared\n",
    "            dat2 = c(colnames(WeeklyRet)[i], colnames(trends[j]),\n",
    "                     colnames(trends[l]),\n",
    "                     signif(sumz3$coefficients[1:4,1], 3),\n",
    "                     signif(sumz3$coefficients[1:4,3], 3),\n",
    "                     signif(sumz3$adj.r.squared, 3),\n",
    "                     round(imp, 3)) \n",
    "            twolist[[k2]] = dat2\n",
    "            k2 = k2 + 1\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "tryCatch({\n",
    "OneKey <- data.frame(matrix(unlist(onelist), nrow=length(onelist), byrow=T))\n",
    "colnames(OneKey) <- c(\"Security\", \"Term\", \"beta_0\", \"beta_1\", \"k_1\",\n",
    "                      \"t_beta_0\", \"t_beta_1\", \"t_k_1\", \"Adj.R.Sq\")},\n",
    "error=function(cond) {message(\"These keywords can't help with prediction\")\n",
    "})\n",
    "#OneKey <- OneKey[order(OneKey$Adj.R.Sq,decreasing = TRUE),]\n",
    "\n",
    "tryCatch({\n",
    "TwoKey <- data.frame(matrix(unlist(twolist), nrow=length(twolist), byrow=T))\n",
    "colnames(TwoKey) <- c(\"Security\", \"Term1\", \"Term2\", \"beta_0\", \"beta_1\", \"k_1\", \"k_2\",\n",
    "                      \"t_beta_0\", \"t_beta_1\", \"t_k_1\", \"t_k_2\",\n",
    "                      \"Adj.R.Sq\", \"improvement\")},\n",
    "error=function(cond) {message(\"These keywords can't help with prediction\")\n",
    "})\n",
    "TwoKey <- TwoKey[order(TwoKey$Adj.R.Sq,decreasing = TRUE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
