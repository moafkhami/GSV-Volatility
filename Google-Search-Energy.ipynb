{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google search keywords to predict energy price volatiltiy\n",
    "\n",
    "This notebook provides a summary and the step-by-step execution of [Google search keywords that best predict energy price volatility](https://www.sciencedirect.com/science/article/pii/S0140988317302517). Please see the paper's first section for a thorough introduction and background of the questions addressed. I have made the complete code also available in the same Github repository.\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Internet search activity data has been widely used as an instrument to approximate trader attention in dif- ferent markets. This method has proven effective in predicting market indices in the short-term. However, little attention has been paid to demonstrating search activity for keywords that best grab investor attention in different markets. This study attempts to build the best practically possible proxy for attention in the market for energy commodities using Google search data. Specifically, we confirm the utility of Google search activity for energy related keywords are significant predictors of volatility by showing they have incremental predictive power beyond the conventional GARCH models in predicting volatility for energy commodities’ prices. Starting with a set of ninety terms used in the energy sector, the study uses a multistage filtering pro- cess to create combinations of keywords that best predict the volatility of crude oil (Brent and West Texas Intermediate), conventional gasoline (New York Harbor and US Gulf Coast), heating oil (New York Harbor), and natural gas prices. For each commodity, combinations that enhance GARCH most effectively are established as proxies of attention. The results indicate investor attention is widely reflected in Internet search activities and demonstrate search data for what keywords best reveal the direction of concern and attention in energy markets.\n",
    "\n",
    "## Introduction\n",
    "One of the most commonly accepted explanations of the observed patterns of volatility is that volatility is proportional to the rate of information inflows and investor attention. Relying on this theory, many previous studies have relied on using Google Search Volume data as a proxy for retail investor attention. The figure below illustrates  the comovement of energy price volatility and Google Search Volume data. \n",
    "![fig2](GSVimg/fig2.png)\n",
    "The question we are seeking to answer is what proxies (i.e., which keywords) best capture the investor attention in the energy market.\n",
    "\n",
    "## Data\n",
    "We require two sets of data. Google Search volume and Energy commodity prices.\n",
    "\n",
    "#### Google Search Volume\n",
    "Google search time series data are downloaded for each keyword *separately* from [Google Trends](https://trends.google.com). The frequency of the data is weekly and the time window is Jan 2004 - July 2016.\n",
    "The energy related keywords provided in Table 1.\n",
    "![table1](GSVimg/table1.png)\n",
    "\n",
    "#### Energy market data\n",
    "Daily and weekly spot prices for crude oil (Brent and West Texas Intermediate), conventional gasoline (New York Harbor and US Gulf Coast), heating oil (New York Harbor), and natural gas for the weeks ending in Friday are downloaded from the Energy Information Administration (EIA) website. Returns and annualized weekly volatilities are calculated and the data are then combined into a single data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- read.csv(\"PricesAd.csv\")\n",
    "df$Date <- as.Date( as.character(df$Date), \"%d-%b-%y\")\n",
    "\n",
    "df <- subset(df, as.Date(\"2004-01-01\") < Date & Date < as.Date(\"2016-07-24\"))\n",
    "\n",
    "#returns\n",
    "returns <- diff(as.matrix(log(df[,-1])))\n",
    "returns <- data.frame(df$Date[-1], returns)\n",
    "colnames(returns)[1] <- c(\"Date\")\n",
    "\n",
    "#weekly volatilites\n",
    "library(xts)\n",
    "colSd <- function (x, na.rm=FALSE) {apply(X=x, MARGIN=2,\n",
    "                                          FUN=sd, na.rm=na.rm)}\n",
    "returns <- as.xts(returns[, 2:ncol(returns)], order.by = as.Date(returns$Date))\n",
    "volatilities <- apply.weekly(returns, colSd)\n",
    "volatilities <- data.frame(Date=index(volatilities),\n",
    "                           coredata(volatilities))\n",
    "\n",
    "#merge with trends data\n",
    "trends <- read.csv(\"trends.csv\")\n",
    "df <- cbind(volatilities, trends[,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "The methodology consists of two major parts. At first, we refine the set of keywords to keep only the terms that both Granger cause the volatility of prices and have incremental predictive power beyond the conventional GARCH model. Second, we build proxies for attention using combinations of these keywords that have predictive power beyond models using fewer keywords and have an improved adjusted-$R^2$ compared to other models. This allows us to create proxies that best explain volatility and are thus suitable proxies for attention.\n",
    "\n",
    "#### Granger Causality\n",
    "\n",
    "For the GSV of a keyword to be a true representative of at least some investors’ attention, it needs to be verified that the GSV leads volatility changes. This can be examined using the Granger Causality test. However, before proceeding with conducting the test, we should make sure that our time series are not unit-root processes. ADF test is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tseries)\n",
    "suppressWarnings(apply(volatilities[2:ncol(volatilities)], 2, adf.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ADF results, the null hypothesis that a unit-root is present is rejected for all six volatility series at $1\\%$ significance level. Note that although the null is not rejected for all GSV series, we are still capable of running the Granger causality test as in each test one of the variables is stationary. For the keywords and weekly volatiltiy data, the following vector autoregression models are constructed:\n",
    "$$\n",
    "V_t = c+ \\sum_{i=1}^{p}\\beta_{1i}V_{t-i} + \\sum_{j=1}^{q} \\beta_{2j}G_{t-j} + \\epsilon_t\n",
    "$$\n",
    "Lag length is set to 2 for all models. The null hypothesis ($H_0$) that $GSV_t$ does not Granger Cause $V_t$ is tested using the F-test. In other terms:\n",
    "$$\n",
    "H_0: \\beta_{2j}=0 \\quad j=1,2,...,q\n",
    "$$\n",
    "The rejection of null-hypothesis for keyword y indicates $G_{y,t}$ can be considered to Granger cause $V_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lmtest)\n",
    "GrangerTable <- matrix(NA, nrow = ncol(trends)-1, ncol = ncol(volatilities)-1)\n",
    "for (i in 1:nrow(GrangerTable)){\n",
    "  for (j in 1:ncol(GrangerTable)){\n",
    "    pval = grangertest(df[,j+1] ~ df[,i+ncol(GrangerTable)+1], order=2, data=df)$Pr[2]\n",
    "    GrangerTable[i,j] = ifelse(pval < 0.05, signif(pval, digits = 2), \"--\")\n",
    "    #GrangerTable[i,j] = grangertest(df[,j+1] ~ df[,i+ncol(GrangerTable)+1], order=2, data=df)$Pr[2]\n",
    "  }\n",
    "}\n",
    "colnames(GrangerTable) <- colnames(volatilities)[-1]\n",
    "rownames(GrangerTable) <- colnames(trends)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the GARCH models\n",
    "The Garch models are set-up according to the two step approach of Engle and Sheppard. This method circumvents the difficulties caused by dimensionality and allows for hypothesis testing using ordinary methods. With $a_t = r_t − \\mu_t$ being the return innovation of week $t$ and letting $a_t$ follow a GARCH(1,1) process, we have $$a_t=\\sqrt{h_t}\\epsilon_t,$$ where $h_t$ is a process such that \n",
    "$$\n",
    "h_t=\\omega+\\gamma a_{t-1}^2+\\beta h_{t-1}\n",
    "$$\n",
    "This equation is estimated using the method of maximum likelihood with Student’s t-distributed errors to take into account the excess kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(fGarch)\n",
    "WeeklyRet <- apply.weekly(returns, mean)\n",
    "#GARCH (1,1) model\n",
    "\n",
    "GarchEst <- matrix(NA, nrow = 2*ncol(WeeklyRet), ncol = 7)\n",
    "\n",
    "#Residual and cond. var. extraction\n",
    "Params <- matrix(NA, nrow = nrow(WeeklyRet), ncol = 2*ncol(WeeklyRet))\n",
    "for (i in 1:ncol(WeeklyRet)){\n",
    "  fitted <- garchFit(formula = ~garch(1,1), data = WeeklyRet[,i], cond.dist=\"std\")\n",
    "  GarchEst[2*i-1, 1] = colnames(WeeklyRet)[i]\n",
    "  GarchEst[2*i-1, 2:5] = fitted@fit$coef[1:4]\n",
    "  GarchEst[2*i, 2:5] = paste(\"(\",fitted@fit$tval[1:4], \")\")\n",
    "  GarchEst[2*i-1, 6] = fitted@fit$llh\n",
    "  GarchEst[2*i-1, 7] = fitted@fit$ics[1]\n",
    "  Params[, 2*i-1] = log(residuals(fitted)^2)\n",
    "  Params[, 2*i] = fitted@h.t*1000\n",
    "  #Extracting residuals and conditional variance\n",
    "}\n",
    "\n",
    "GarchEst[is.na(GarchEst)] <- \"\"\n",
    "colnames(GarchEst) <- c(\"Commodity\", \"mu\", \"omega\", \"alpha\", \"beta\", \"logL\", \"AIC\" )\n",
    "#xtable(GarchEst)\n",
    "colnames(Params) <- c(1:ncol(Params))\n",
    "\n",
    "for (i in 1:ncol(WeeklyRet)){\n",
    "  colnames(Params)[2*i-1] <- paste0(\"s_\", colnames(WeeklyRet)[i], sep='')\n",
    "  colnames(Params)[2*i] <- paste0(\"v_\", colnames(WeeklyRet)[i], sep='')\n",
    "}\n",
    "\n",
    "df <- cbind (Params[-1,], trends[-ncol(trends),]) #using lagged trends\n",
    "\n",
    "library(dplyr)\n",
    "df <- df %>%\n",
    "  select(Date, everything())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table below shows the GARCH model estimates\n",
    "![table3](GSVimg/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving predictive power beyond GARCH\n",
    "\n",
    "In the second step, for each commodity, the vector of conditional variances, $h_t$ is extracted from the GARCH(1,1) models to be used as the explanatory variable along with the GSV series in the following Ordinary Least Squares regressions for each keyword $y$:\n",
    "$$\n",
    "\\ln(a_t^2) = \\beta_0 + \\beta_1h_{t-1} + k_1 G_{t-1} + z_t\n",
    "$$\n",
    "Newey and West robust standard errors account for any heteroskedasticity and autocorrelation in the residuals ($z_t).\n",
    "The next step of the filtration process is performed by utilizing the developed GARCH model. For all keywords, we test the null hypothesis that the keyword’s GSV has no predictive power beyond GARCH using an F-test. Keywords for which the null is rejected are kept in the set.\n",
    "\n",
    "#### Combining the terms to create a better proxy\n",
    "Similar to the previous stage of the filtration process, in this level, we use F-tests to see whether adding GSV series for an additional keyword enhances our predictive power of shocks beyond the models. We begin by repeating the second step of deriving the GARCH model but this time adding two predictor variables instead of only one:\n",
    "\n",
    "$$\n",
    "\\ln(a_t^2) = \\beta_0 + \\beta_1h_{t-1} + k_1 G_{1,t-1} + k_2 G_{2,t-1} + z_t\n",
    "$$\n",
    "\n",
    "Fixing the first keyword for all equations we test the hypothesis that adding GSV of one more keyword remaining in the set provides no more predictive power. The null is $k_2 = 0$. Combinations of keywords for which this hypothesis is rejected at $5\\%$ significance, and whose OLS parameter estimates are significant at $5\\%$ level pass this stage of filtration. These combinations are considered as predictors with predictive power beyond models that include GSV for only one keyword.\n",
    "\n",
    "The next stage of the filtration process is exactly the same as the previous stage, and we test  the null of $k_3 = 0$ in the equation below:\n",
    "\n",
    "$$\n",
    "\\ln(a_t^2) = \\beta_0 + \\beta_1h_{t-1} + k_1 G_{1,t-1} + k_2 G_{2,t-1} + k_3 G_{3,t-1} + z_t\n",
    "$$\n",
    "\n",
    "To the developed models, new keywords are added in a similar fashion until one of the stopping conditions is met. That is either the model fails to enhance the predictive power or there is no improvement in the adjusted R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F-test and tables 4-5-6\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "onelist <- list()\n",
    "twolist <- list()\n",
    "threelist <- list()\n",
    "k1 = k2 = k3 = 1\n",
    "for (i in 1:ncol(WeeklyRet)){\n",
    "  z1 <- lm(df[,2*i] ~ df[,2*i+1], data = df) #original regression\n",
    "  sumz1 <- summary(z1)\n",
    "  err1 <- coeftest(z1, vcov = vcovHC(z1, type=\"HC1\")) #White Errors\n",
    "  for (j in 2:ncol(trends)){\n",
    "    if (GrangerTable[j - 1, i]!= \"--\"){ #Granger causality condition\n",
    "      z2 <- lm(df[,2*i] ~ df[,2*i + 1] + df[,2*ncol(WeeklyRet) + j], data = df)\n",
    "      sumz2 <- summary(z2) #new equation\n",
    "      err2 <- coeftest(z2, vcov = vcovHC(z2, type=\"HC1\"))\n",
    "      if (anova(z1, z2)$Pr[2] <= 0.05 && err2[3,4] <= 0.05\n",
    "          && sumz2$adj.r.squared > 1.1* sumz1$adj.r.squared){ #enhancement requirements\n",
    "        dat1 = c(colnames(WeeklyRet)[i], colnames(trends[j]),\n",
    "                signif(sumz2$coefficients[1:3,1], 3),\n",
    "                signif(sumz2$coefficients[1:3,3], 3),\n",
    "                signif(sumz2$adj.r.squared, 3)) \n",
    "        onelist[[k1]] = dat1 #append\n",
    "        k1 = k1 + 1\n",
    "        for (l in (j+1):ncol(trends)){ #two keywords\n",
    "          z3 <- lm(df[,2*i] ~ df[,2*i+1] +\n",
    "                     df[,2*ncol(WeeklyRet) + j] + df[,2*ncol(WeeklyRet) + l] ,\n",
    "                   data = df)\n",
    "          sumz3 <- summary(z3)\n",
    "          err3 <- coeftest(z3, vcov = vcovHC(z3, type=\"HC1\"))\n",
    "          if (anova(z2, z3)$Pr[2] <= 0.05 && err3[4,4] <= 0.05\n",
    "              && sumz3$adj.r.squared > 1.1* sumz2$adj.r.squared){ \n",
    "            dat2 = c(colnames(WeeklyRet)[i], colnames(trends[j]),\n",
    "                     colnames(trends[l]),\n",
    "                    signif(sumz3$coefficients[1:4,1], 3),\n",
    "                    signif(sumz3$coefficients[1:4,3], 3),\n",
    "                    signif(sumz3$adj.r.squared, 3)) \n",
    "            twolist[[k2]] = dat2\n",
    "            k2 = k2 + 1\n",
    "            if (l < ncol(trends)){\n",
    "              for (m in (l+1):ncol(trends)){\n",
    "                z4 <- lm(df[,2*i] ~ df[,2*i+1] +\n",
    "                           df[,2*ncol(WeeklyRet) + j] + df[,2*ncol(WeeklyRet) + l] +\n",
    "                           df[,2*ncol(WeeklyRet) + m], data = df)\n",
    "                sumz4 <- summary(z4)\n",
    "                err4 <- coeftest(z4, vcov = vcovHC(z4, type=\"HC1\"))\n",
    "                if (anova(z3, z4)$Pr[2] <= 0.05 && err4[5,4] <= 0.05\n",
    "                    && sumz4$adj.r.squared > 1.1* sumz3$adj.r.squared){\n",
    "                  imp=(100*(sumz4$adj.r.squared-sumz1$adj.r.squared))/sumz1$adj.r.squared\n",
    "                  dat3 = c(colnames(WeeklyRet)[i], colnames(trends[j]),\n",
    "                           colnames(trends[l]),\n",
    "                           colnames(trends[m]),\n",
    "                           signif(sumz4$coefficients[1:5,1], 3),\n",
    "                           signif(sumz4$coefficients[1:5,3], 3),\n",
    "                           signif(sumz4$adj.r.squared, 3),\n",
    "                           #imp)\n",
    "                           round(imp, 3)) \n",
    "                  threelist[[k3]] = dat3\n",
    "                  k3 = k3 + 1\n",
    "                }\n",
    "            }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "OneKey <- data.frame(matrix(unlist(onelist), nrow=length(onelist), byrow=T))\n",
    "colnames(OneKey) <- c(\"Commodity\", \"Term\", \"beta_0\", \"beta_1\", \"k_1\",\n",
    "                      \"t_beta_0\", \"t_beta_1\", \"t_k_1\", \"Adj.R.Sq\")\n",
    "#OneKey <- OneKey[order(OneKey$Adj.R.Sq,decreasing = TRUE),]\n",
    "\n",
    "TwoKey <- data.frame(matrix(unlist(twolist), nrow=length(twolist), byrow=T))\n",
    "colnames(TwoKey) <- c(\"Commodity\", \"Term1\", \"Term2\", \"beta_0\", \"beta_1\", \"k_1\", \"k_2\",\n",
    "                      \"t_beta_0\", \"t_beta_1\", \"t_k_1\", \"t_k_2\", \"Adj.R.Sq\")\n",
    "#TwoKey <- TwoKey[order(TwoKey$Adj.R.Sq,decreasing = TRUE),]\n",
    "\n",
    "ThreeKey <- data.frame(matrix(unlist(threelist), nrow=length(threelist), byrow=T))\n",
    "colnames(ThreeKey) <- c(\"Commodity\", \"Term1\", \"Term2\", \"Term3\", \"beta_0\", \"beta_1\",\n",
    "                      \"k_1\", \"k_2\", \"k_3\", \"t_beta_0\", \"t_beta_1\", \"t_k_1\", \"t_k_2\",\n",
    "                      \"t_k_3\", \"Adj.R.Sq\", \"improvement\")\n",
    "\n",
    "ThreeKey <- ThreeKey[order(ThreeKey$Adj.R.Sq, decreasing = TRUE),]\n",
    "#rm(onelist, twolist, threelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This able show the best combinations of keywords for predicting volatiltiy for each energy commodity.\n",
    "![table6](GSVimg/table6.png)\n",
    "\n",
    "Based on the significance of estimates and the magnitude of the adjusted $R^2$, combination of GSV of keywords presented in table below are considered as the best proxies for investor attention. The second column shows the improvement obtained as compared to the GARCH models.\n",
    "![table7](GSVimg/table7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
